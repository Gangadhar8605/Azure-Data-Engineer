population_df = spark.read.format('csv').\
    option('header', 'true').\
    option("sep", '\t'). \
    load('abfss://bronze@project1adls.dfs.core.windows.net/population')

from pyspark.sql.functions import regexp_replace, col
for i in population_df.columns[1:]:
    population_df1 = population_df.withColumn(i, regexp_replace(col(i), "[^0-9.]", ""))

population_unpivot_df = population_df1.unpivot("indic_de,geo\\time", population_df.columns[1:], "year", "percentage")

from pyspark.sql.functions import split, col
population_final_df = population_unpivot_df.withColumn("age_group",regexp_replace(split(population_unpivot_df["indic_de,geo\\time"],',')[0],'PC_','')).withColumn("geo",split(population_unpivot_df["indic_de,geo\\time"],',')[1]).drop("indic_de,geo\\time")


schema_ = 'country string,country_code_2_digit string,country_code_3_digit string,continent string,population int'
country_lookup_df = spark.read.csv('abfss://dimfiles@project1adls.dfs.core.windows.net/country_lookup.csv',header=True,schema=schema_)

population_2019_df = population_final_df.where('year = 2019')
population_data = population_2019_df.join(country_lookup_df, population_2019_df.geo == country_lookup_df.country_code_2_digit, 'left')
population_data.write.format('delta').mode('overwrite').save('abfss://silver@project1adls.dfs.core.windows.net/population')
